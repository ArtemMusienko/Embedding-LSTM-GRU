{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa9Cjbj40hsv"
      },
      "source": [
        "# Решение задачи"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Представленная задача решает ***следующую задачу:*** модель получает в качестве входных данных текст или текстовый файл, на основе которого происходит предсказание и выдача результата в виде процента схожести по стилистике написания текста в сравнении с  4 русскими авторами."
      ],
      "metadata": {
        "id": "npcpyJDQIEeF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Установим Gradio, который будет учавствовать в создании интерфейса. Выполним установку токенизатора Razdel, загрузим предобученные Embedding. И установим большую базу текстов на русском языке - navec:"
      ],
      "metadata": {
        "id": "PJenLDp7F6Bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gradio\n",
        "!pip install razdel\n",
        "!wget https://storage.yandexcloud.net/natasha-navec/packs/navec_hudlit_v1_12B_500K_300d_100q.tar\n",
        "!pip install navec"
      ],
      "metadata": {
        "id": "n55-havNDGk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b087d17-9c1c-4ea3-c92b-9dddc4b6313a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.38.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting brotli>=1.1.0 (from gradio)\n",
            "  Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Collecting gradio-client==1.11.0 (from gradio)\n",
            "  Downloading gradio_client-1.11.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.38.0-py3-none-any.whl (59.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.11.0-py3-none-any.whl (324 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.5/324.5 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: brotli, gradio-client, gradio\n",
            "  Attempting uninstall: gradio-client\n",
            "    Found existing installation: gradio_client 1.10.1\n",
            "    Uninstalling gradio_client-1.10.1:\n",
            "      Successfully uninstalled gradio_client-1.10.1\n",
            "  Attempting uninstall: gradio\n",
            "    Found existing installation: gradio 5.31.0\n",
            "    Uninstalling gradio-5.31.0:\n",
            "      Successfully uninstalled gradio-5.31.0\n",
            "Successfully installed brotli-1.1.0 gradio-5.38.0 gradio-client-1.11.0\n",
            "Collecting razdel\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: razdel\n",
            "Successfully installed razdel-0.5.0\n",
            "--2025-07-18 13:49:51--  https://storage.yandexcloud.net/natasha-navec/packs/navec_hudlit_v1_12B_500K_300d_100q.tar\n",
            "Resolving storage.yandexcloud.net (storage.yandexcloud.net)... 213.180.193.243, 2a02:6b8::1d9\n",
            "Connecting to storage.yandexcloud.net (storage.yandexcloud.net)|213.180.193.243|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53012480 (51M) [application/x-tar]\n",
            "Saving to: ‘navec_hudlit_v1_12B_500K_300d_100q.tar’\n",
            "\n",
            "navec_hudlit_v1_12B 100%[===================>]  50.56M  15.1MB/s    in 3.7s    \n",
            "\n",
            "2025-07-18 13:49:56 (13.6 MB/s) - ‘navec_hudlit_v1_12B_500K_300d_100q.tar’ saved [53012480/53012480]\n",
            "\n",
            "Collecting navec\n",
            "  Downloading navec-0.10.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from navec) (2.0.2)\n",
            "Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: navec\n",
            "Successfully installed navec-0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импортируем необходимые библиотеки и предопределим некоторые переменные:"
      ],
      "metadata": {
        "id": "m9zP6bhAF6be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from navec import Navec\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Embedding, Dense, SpatialDropout1D, BatchNormalization, Dropout, Bidirectional, LSTM, GRU\n",
        "import keras\n",
        "from razdel import tokenize\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import gradio as gr\n",
        "import os\n",
        "\n",
        "WIN_SIZE = 1000 #размер окна\n",
        "WIN_STEP = 100 #шаг"
      ],
      "metadata": {
        "id": "eOxX_scWFk3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Воспользуемся новым методом для нас загрузки датасета, описанным в официальной документации **Keras**. Данный метод позволяет избежать повторной загрузки, если данные были раньше скачаны. Метод возвращает путь к папке с датасетом.\n",
        "\n",
        "Разархивируем датасета во временную папку *dataset:*\n",
        "\n",
        "Отберем для обучения 12 писателей:"
      ],
      "metadata": {
        "id": "yABkAIMuF6ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = keras.utils.get_file(\n",
        "    \"russian_literature.zip\",\n",
        "    \"https://storage.yandexcloud.net/academy.ai/russian_literature.zip\"\n",
        ")\n",
        "\n",
        "!unzip -qo \"{data_path}\" -d ./dataset\n",
        "\n",
        "CLASS_LIST=[\"Dostoevsky\", \"Tolstoy\", \"Turgenev\", \"Chekhov\", \"Lermontov\", \"Blok\", \"Pushkin\", \"Gogol\", \"Gorky\", \"Herzen\", \"Bryusov\", \"Nekrasov\" ]"
      ],
      "metadata": {
        "id": "JvmB1sfxFm3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcf79d06-7c7d-4df0-9cac-7e711afca993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.yandexcloud.net/academy.ai/russian_literature.zip\n",
            "\u001b[1m21547079/21547079\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "error:  cannot create ./dataset/poems/Blok/╨б╤В╨╕╤Е╨╛╤В╨▓╨╛╤А╨╡╨╜╨╕╤П 1897-1903 ╨│╨│, ╨╜╨╡ ╨▓╨╛╤И╨╡╨┤╤И╨╕╨╡ ╨▓ ╨╛╤Б╨╜╨╛╨▓╨╜╨╛╨╡ ╤Б╨╛╨▒╤А╨░╨╜╨╕╨╡.txt\n",
            "        File name too long\n",
            "error:  cannot create ./dataset/prose/Gogol/╨Я╨╛╨▓╨╡╤Б╤В╤М ╨╛ ╤В╨╛╨╝, ╨║╨░╨║ ╨┐╨╛╤Б╤Б╨╛╤А╨╕╨╗╤Б╤П ╨Ш╨▓╨░╨╜ ╨Ш╨▓╨░╨╜╨╛╨▓╨╕╤З ╤Б ╨Ш╨▓╨░╨╜╨╛╨╝ ╨Э╨╕╨║╨╕╤Д╨╛╤А╨╛╨▓╨╕╤З╨╡╨╝.txt\n",
            "        File name too long\n",
            "error:  cannot create ./dataset/publicism/Tolstoy/╨Ф╨╛╨║╨╗╨░╨┤, ╨┐╤А╨╕╨│╨╛╤В╨╛╨▓╨╗╨╡╨╜╨╜╤Л╨╣ ╨┤╨╗╤П ╨║╨╛╨╜╨│╤А╨╡╤Б╤Б╨░ ╨╛ ╨╝╨╕╤А╨╡ ╨▓ ╨б╤В╨╛╨║╨│╨╛╨╗╤М╨╝╨╡.txt\n",
            "        File name too long\n",
            "error:  cannot create ./dataset/publicism/Tolstoy/╨Я╨╛╤З╨╡╨╝╤Г ╤Е╤А╨╕╤Б╤В╨╕╨░╨╜╤Б╨║╨╕╨╡ ╨╜╨░╤А╨╛╨┤╤Л ╨▓╨╛╨╛╨▒╤Й╨╡ ╨╕ ╨▓ ╨╛╤Б╨╛╨▒╨╡╨╜╨╜╨╛╤Б╤В╨╕ ╤А╤Г╤Б╤Б╨║╨╕╨╣ ╨╜╨░╤Е╨╛╨┤╤П╤В╤Б╤П ╤В╨╡╨┐╨╡╤А╤М ╨▓ ╨▒╨╡╨┤╤Б╤В╨▓╨╡╨╜╨╜╨╛╨╝ ╨┐╨╛╨╗╨╛╨╢╨╡╨╜╨╕╨╕.txt\n",
            "        File name too long\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Следующий код собирает текстовые данные из файлов в структуру словаря *all_texts*, где *ключи* — это имена авторов, а *значения* — объединённые тексты всех их произведений."
      ],
      "metadata": {
        "id": "XqHwmfMNF7ET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_texts = {} #собираем в словарь весь датасет\n",
        "\n",
        "for author in CLASS_LIST:\n",
        "    all_texts[author] = '' #инициализируем пустой строкой новый ключ словаря\n",
        "    for path in glob.glob('./dataset/prose/{}/*.txt'.format(author)) +  glob.glob('./dataset/poems/{}/*.txt'.format(author)): #поиск файлов по шаблону\n",
        "        with open(f'{path}', 'r', errors='ignore') as f: #игнорируем ошибки (например символы из другой кодировки)\n",
        "            text = f.read() #загрузка содержимого файла в строку\n",
        "\n",
        "        all_texts[author]  += ' ' + text.replace('\\n', ' ') #заменяем символ перехода на новую строку пробелом"
      ],
      "metadata": {
        "id": "0IZv0hnuFoUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Далее выполним подсчёт популярности слов (токенов) в текстах из словаря *all_texts* и создадим словарь наиболее частых токенов (*word_index*), который можно использовать для векторизации текста.\n",
        "\n",
        "**Razdel** — это библиотека для токенизации русского текста (разбиения на слова и знаки препинания).\n",
        "\n",
        "Функция `razdel_tokenize(text)` принимает текст и возвращает список токенов (отдельных слов и символов)."
      ],
      "metadata": {
        "id": "5MBJ-OpQF7ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#реализация токенизатора Razdel\n",
        "def razdel_tokenize(text):\n",
        "    return [token.text for token in tokenize(text)]\n",
        "\n",
        "token_counts = defaultdict(int)\n",
        "max_words = 15000 #укажем размер словаря\n",
        "\n",
        "#подсчет частотности токенов\n",
        "for text in all_texts.values():\n",
        "    tokens = razdel_tokenize(text.lower())\n",
        "    for token in tokens:\n",
        "        token_counts[token] += 1\n",
        "\n",
        "sorted_tokens = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)[:max_words ]#сортируем токены по частоте и берем top-N\n",
        "word_index = {word: i+1 for i, (word, count) in enumerate(sorted_tokens)} #+1 потому что 0 зарезервирован"
      ],
      "metadata": {
        "id": "5bQM8nZaaV54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Преобразуем тексты из словаря *all_texts* в числовые последовательности, используя ранее созданный словарь *word_index*. Каждое слово заменяется на соответствующий ему числовой индекс, а неизвестные слова (которых нет в *word_index*) заменяются на 0:"
      ],
      "metadata": {
        "id": "H9wNiBUIbAGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_train = []\n",
        "for text in all_texts.values():\n",
        "    tokens = razdel_tokenize(text)\n",
        "    seq = [word_index.get(token, 0) for token in tokens if word_index.get(token, 0) < max_words]\n",
        "    seq_train.append(seq)\n",
        "\n",
        "total = sum(len(i) for i in seq_train) #вычисляем общее количество слов в датасете:\n",
        "\n",
        "#общая выборка по писателям, их доля в базе данных, среднее и медианное значение слов\n",
        "mean_list = np.array([])\n",
        "for author in CLASS_LIST:\n",
        "    cls = CLASS_LIST.index(author)\n",
        "    mean_list = np.append(mean_list, len(seq_train[cls])) #собирает данные о длинах текстов для последующей балансировки датасета\n",
        "\n",
        "#сбалансированная выборка\n",
        "median = int(np.median(mean_list)) #зафиксировали медианное значение\n",
        "seq_train_balance = []\n",
        "CLASS_LIST_BALANCE = []\n",
        "for author in CLASS_LIST:\n",
        "    cls = CLASS_LIST.index(author)\n",
        "    if len(seq_train[cls]) > median * 0.6:\n",
        "        seq_train_balance.append(seq_train[cls][:median])\n",
        "        CLASS_LIST_BALANCE.append(author)\n",
        "\n",
        "#cлучайный выбор 4 писателей из сбалансированного списка\n",
        "selected_authors = random.sample(CLASS_LIST_BALANCE, 4)\n",
        "selected_seq_train_balance = [seq_train_balance[CLASS_LIST_BALANCE.index(author)] for author in selected_authors]"
      ],
      "metadata": {
        "id": "sLWRYm3nFqXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Далее реализуем 2 функции, а именно: **seq_split**, которая отвечает за разбиение последовательности чисел на небольшие \"окна\" фиксированной длины и **seq_vectorize**, которая осуществляет подготовку данных для нейросетей с разделением на обучение/валидацию/тест:"
      ],
      "metadata": {
        "id": "uOhrzyoNF8BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_split(sequence, win_size, step):\n",
        "    return [sequence[i:i + win_size] for i in range(0, len(sequence) - win_size + 1, step)]\n",
        "\n",
        "def seq_vectorize(seq_list, test_split, val_split, class_list, win_size, step):\n",
        "    x_train, y_train = [], []\n",
        "    x_val, y_val = [], []\n",
        "    x_test, y_test = [], []\n",
        "\n",
        "    #обрабатываем каждую последовательность\n",
        "    for class_idx, seq in enumerate(seq_list):\n",
        "        #вычисляем границы разделения последовательности\n",
        "        total_len = len(seq)\n",
        "        val_end = int(total_len * (1 - test_split - val_split))\n",
        "        test_end = int(total_len * (1 - test_split))\n",
        "\n",
        "        #разбиваем последовательность на окна для каждой части\n",
        "        train_windows = seq_split(seq[:val_end], win_size, step)\n",
        "        val_windows = seq_split(seq[val_end:test_end], win_size, step)\n",
        "        test_windows = seq_split(seq[test_end:], win_size, step)\n",
        "\n",
        "        #добавляем окна в соответствующие списки\n",
        "        x_train += train_windows\n",
        "        x_val += val_windows\n",
        "        x_test += test_windows\n",
        "\n",
        "        #создаем one-hot метки и добавляем их в соответствующие списки\n",
        "        #количество меток равно количеству окон\n",
        "        y_train += [keras.utils.to_categorical(class_idx, len(class_list))] * len(train_windows)\n",
        "        y_val += [keras.utils.to_categorical(class_idx, len(class_list))] * len(val_windows)\n",
        "        y_test += [keras.utils.to_categorical(class_idx, len(class_list))] * len(test_windows)\n",
        "\n",
        "    #преобразуем списки в numpy массивы и возвращаем результат\n",
        "    return (np.array(x_train), np.array(y_train),\n",
        "            np.array(x_val), np.array(y_val),\n",
        "            np.array(x_test), np.array(y_test))"
      ],
      "metadata": {
        "id": "b5kYK3hAFrkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выполним финальную подготовку модели, создав функцию `build_and_train_model`.\n",
        "\n",
        "Преобразуем тексты в числовой формат и разделяя их на обучающую, валидационную и тестовую выборки.\n",
        "\n",
        "Загрузим предобученную модель, инициализируем матрицу эмбеддингов и заполним матрицы векторами слов:\n",
        "\n",
        "`hudlit_12B_500K_300d_100q` — это **GloVe-эмбединги** обученные на 145ГБ художественной литературы. Архив с текстами взят из проекта **RUSSE** и использует оригинальную реализацию **GloVe** на C, обернутую в удобный Python-интерфейс.\n",
        "\n",
        "Размер словаря `hudlit_12B_500K_300d_100q` — 500 000 записей, он покрывает 98% слов в художественных текстах. Оптимальная размерность векторов — 300. Таблица 500 000 × 300 из float-чисел занимает 578МБ, размер архива с весами `hudlit_12B_500K_300d_100q` в 12 раз меньше (48МБ). И это благодаря квантизации.\n",
        "\n",
        "Выполним настройку архитектуры добавим callbacks, осуществим компиляцию модели, её обучение и дальнейшее сохранение:"
      ],
      "metadata": {
        "id": "cnZFbj18F8uv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = None #глобальная переменная для хранения модели\n",
        "\n",
        "def build_and_train_model():\n",
        "    global model\n",
        "    x_train, y_train, x_val, y_val, x_test, y_test = seq_vectorize(\n",
        "        selected_seq_train_balance,\n",
        "        test_split=0.1,\n",
        "        val_split=0.1,\n",
        "        class_list=selected_authors,\n",
        "        win_size=WIN_SIZE,\n",
        "        step=WIN_STEP\n",
        "    )\n",
        "    navec = Navec.load('navec_hudlit_v1_12B_500K_300d_100q.tar')\n",
        "    embedding_matrix = np.zeros((max_words, 300))\n",
        "    for word, i in word_index.items():\n",
        "        if i < max_words:\n",
        "            embedding_vector = navec.get(word)\n",
        "            if embedding_vector is not None: #слова, которых нет в Navec (или с индексом >= max_words), получают нулевой вектор\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "    model_local = Sequential()\n",
        "    model_local.add(Embedding(max_words, 300, input_length=WIN_SIZE, weights=[embedding_matrix]))\n",
        "    model_local.add(SpatialDropout1D(0.3))\n",
        "    model_local.add(BatchNormalization())\n",
        "    model_local.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
        "    model_local.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
        "    model_local.add(Dropout(0.3))\n",
        "    model_local.add(BatchNormalization())\n",
        "    model_local.add(GRU(32, return_sequences=True, reset_after=True))\n",
        "    model_local.add(GRU(32, reset_after=True))\n",
        "    model_local.add(Dropout(0.3))\n",
        "    model_local.add(BatchNormalization())\n",
        "    model_local.add(Dense(100, activation='relu'))\n",
        "    model_local.add(Dropout(0.3))\n",
        "    model_local.add(BatchNormalization())\n",
        "    model_local.add(Dense(len(selected_authors), activation='softmax'))\n",
        "    x_train_final, x_val_final, y_train_final, y_val_final = train_test_split(\n",
        "        x_train, y_train, test_size=0.1, random_state=42, stratify=y_train\n",
        "    )\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=3,\n",
        "        restore_best_weights=True,\n",
        "        verbose=0\n",
        "    )\n",
        "    reduce_lr = ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.3,\n",
        "        patience=3,\n",
        "        verbose=0\n",
        "    )\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        'best_model.keras',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max',\n",
        "        verbose=0\n",
        "    )\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "    model_local.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    model_local.fit(\n",
        "        x_train_final, y_train_final,\n",
        "        epochs=30,\n",
        "        batch_size=128,\n",
        "        validation_data=(x_val_final, y_val_final),\n",
        "        callbacks=[early_stopping, reduce_lr, checkpoint],\n",
        "        verbose=0\n",
        "    )\n",
        "    model_local = load_model('best_model.keras')\n",
        "    model = model_local"
      ],
      "metadata": {
        "id": "wyf7FUQhFvlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим функцию для интерфейса Gradio, которая запускает предыдущую функцию и сигнализирует об успешном завершении обучения:"
      ],
      "metadata": {
        "id": "rfw6-kxVF9aI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_gradio():\n",
        "    build_and_train_model()\n",
        "    return gr.update(value=\"Обучение завершено!\", interactive=False), gr.update(interactive=True)"
      ],
      "metadata": {
        "id": "98GAAVLFFxKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Общее назначение функции:***\n",
        "\n",
        "Функция принимает текст (или файл), обрабатывает его, преобразует в формат для нейросети, запускает предсказание и возвращает вероятности того, на какого из писателей больше всего похож представленный текст\n",
        "\n",
        "***Основные этапы:***\n",
        "\n",
        "1.   Проверка, обучена ли модель;\n",
        "2.   Получение текста для анализа.;\n",
        "3.   Проверка минимальной длины текста;\n",
        "4.   Токенизация и преобразование в индексы;\n",
        "5.   Формирование окон для подачи на модель;\n",
        "6.   Получение и усреднение предсказаний;\n",
        "7.   Формирование и возврат результата.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aLRxqF2eF9s2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_author(text, file):\n",
        "    global model  #используем глобальную переменную model, чтобы проверить, обучена ли модель и сделать предсказание\n",
        "    if model is None:\n",
        "        #если модель еще не обучена, возвращаем нулевые вероятности для всех авторов\n",
        "        return {author: 0.0 for author in selected_authors}\n",
        "\n",
        "    #получаем текст для анализа: приоритет — текст из поля, иначе из загруженного файла\n",
        "    if text and text.strip() != \"\":\n",
        "        content = text\n",
        "    elif file is not None:\n",
        "        #пытаемся прочитать содержимое файла\n",
        "        try:\n",
        "            with open(file.name, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "        except Exception as e:\n",
        "            #в случае ошибки при чтении файла возвращаем нулевые вероятности\n",
        "            return {author: 0.0 for author in selected_authors}\n",
        "    else:\n",
        "        #если не был передан ни текст, ни файл — возвращаем нулевые вероятности\n",
        "        return {author: 0.0 for author in selected_authors}\n",
        "\n",
        "    #проверяем, что текст достаточно длинный для анализа (минимум 500 символов)\n",
        "    if len(content) < 500:\n",
        "        return {author: 0.0 for author in selected_authors}\n",
        "\n",
        "    #токенизация текста (разделяем на слова)\n",
        "    tokens = razdel_tokenize(content.lower())\n",
        "    #преобразование токенов в числовые индексы для подачи на вход модели\n",
        "    seq = [word_index.get(token, 0) for token in tokens]\n",
        "\n",
        "    #формируем окна фиксированной длины для подачи на модель\n",
        "    if len(seq) < WIN_SIZE:\n",
        "        #если текст слишком короткий, дополняем последовательность нулями до нужной длины\n",
        "        seq_padded = seq + [0] * (WIN_SIZE - len(seq))\n",
        "        windows = [seq_padded]\n",
        "    else:\n",
        "        #если текст достаточно длинный, делим на окна с шагом WIN_STEP\n",
        "        windows = []\n",
        "        for i in range(0, len(seq) - WIN_SIZE + 1, WIN_STEP):\n",
        "            windows.append(seq[i:i + WIN_SIZE])\n",
        "            if len(windows) >= 10:  #ограничиваем количество окон для ускорения инференса\n",
        "                break\n",
        "\n",
        "    windows = np.array(windows)  #переводим список окон в numpy-массив\n",
        "\n",
        "    #получаем предсказания модели для каждого окна\n",
        "    predictions = model.predict(windows, verbose=0)\n",
        "    #усредняем вероятности по всем окнам текста\n",
        "    avg_probs = np.mean(predictions, axis=0)\n",
        "\n",
        "    #формируем результат: словарь {автор: вероятность}\n",
        "    result = {selected_authors[i]: float(avg_probs[i]) for i in range(len(selected_authors))}\n",
        "    return result  #возвращаем вероятности для каждого автора"
      ],
      "metadata": {
        "id": "fWF5ZakrFzpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Далее генерируются примеры текстов для тестирования, создается описание для пользователя, добавляются в интерфейс кнопки для обучения и предсказания, а так же поля для ввода текста/файла и блок с примерами:"
      ],
      "metadata": {
        "id": "XnpK7eznF-Iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#формируем список примеров для интерфейса Gradio\n",
        "examples = []\n",
        "for author in selected_authors:\n",
        "    text_sample = all_texts[author]  #берем полный текст автора\n",
        "    #случайно выбираем фрагмент длиной 1000 символов для примера\n",
        "    start_idx = random.randint(0, len(text_sample) - 1000)\n",
        "    examples.append([text_sample[start_idx:start_idx + 1000], None])\n",
        "\n",
        "#описание интерфейса и инструкции для пользователя\n",
        "description = f\"\"\"\n",
        "## Классификатор авторства литературных текстов\n",
        "Модель определяет авторство среди {len(selected_authors)} русских писателей:\n",
        "{', '.join(selected_authors)}\n",
        "\n",
        "**Инструкция:**\n",
        "1. Нажмите \"Обучить модель\", дождитесь завершения обучения;\n",
        "2. Введите текст в поле ниже (минимум 500 символов) или загрузите файл;\n",
        "3. Нажмите кнопку \"Выполнить предсказание\";\n",
        "4. Результат покажет вероятности для каждого автора.\n",
        "\n",
        "**Советы:**\n",
        "- Используйте примеры ниже для тестирования;\n",
        "- Чем длиннее текст (1-2 тыс. символов), тем точнее результат.\n",
        "\"\"\"\n",
        "\n",
        "#создаем интерфейс Gradio в блоке\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as iface:\n",
        "    gr.Markdown(\"# Классификатор литературных текстов\")\n",
        "    gr.Markdown(description)\n",
        "    #кнопка обучения и статус - первая строка интерфейса\n",
        "    with gr.Row():\n",
        "        btn_train = gr.Button(\"Обучить модель\")\n",
        "        train_status = gr.Textbox(label=\"Статус обучения\", value=\"Модель не обучена\", interactive=False)\n",
        "    #ввод текста или загрузка файла - вторая строка интерфейса\n",
        "    with gr.Row():\n",
        "        input_text = gr.Textbox(\n",
        "            lines=8,\n",
        "            placeholder=\"Введите текст здесь...\",\n",
        "            label=\"Текст для анализа\"\n",
        "        )\n",
        "        input_file = gr.File(label=\"Или загрузите текстовый файл (.txt)\")\n",
        "    #блок вывода результата\n",
        "    output_label = gr.Label(\n",
        "        label=\"Результаты распознавания\",\n",
        "        num_top_classes=3\n",
        "    )\n",
        "    #кнопка для запуска предсказания (неактивна до обучения)\n",
        "    btn_predict = gr.Button(\"Выполнить предсказание\", interactive=False)\n",
        "    #примеры для быстрого теста интерфейса\n",
        "    gr.Examples(\n",
        "        examples=examples,\n",
        "        inputs=[input_text, input_file],\n",
        "        outputs=output_label,\n",
        "        label=\"Примеры\",\n",
        "        examples_per_page=4\n",
        "    )\n",
        "    #обработка нажатия кнопки обучения: запускает обучение модели\n",
        "    btn_train.click(\n",
        "        fn=train_model_gradio,\n",
        "        inputs=[],\n",
        "        outputs=[train_status, btn_predict]\n",
        "    )\n",
        "    #обработка нажатия кнопки предсказания: запускает инференс\n",
        "    btn_predict.click(\n",
        "        fn=predict_author,\n",
        "        inputs=[input_text, input_file],\n",
        "        outputs=output_label\n",
        "    )\n",
        "\n",
        "#запуск интерфейса Gradio\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "0Kp6FOPIcuF_",
        "outputId": "0048ce1e-4947-4fa5-c758-0e071690d165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://29a75c57e8f45ffc1d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://29a75c57e8f45ffc1d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}